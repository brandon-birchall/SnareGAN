{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of SnareGAN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOx/CazTkMk6Xqhbw7TS0aN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/brandon-birchall/SnareGAN/blob/main/SnareGAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mvOlE1N_Pgj6"
      },
      "source": [
        "# GAN Snare Sounds\n",
        "A journey into the usage of a DCGAN to create snare sounds. A for fun project.\n",
        "\n",
        "## Design\n",
        "\n",
        "My initial thoughts are to take an arbitrary but small dimensional vector of noise and to scale it using a generator to a tensor which represents many fourier series which when converted back to the time domain and played in sequence, should represent a snare sound (if the generator is trained enough)\n",
        "\n",
        "The discriminator should determine whether the input fourier series' is from a real snare or a generated snare. This will determine the loss of both the generator and the discriminator. As such, this is a form of zero sum game. \n",
        "\n",
        "In terms of specifics, I will follow a conventional GCGAN architecture: a fully connected layer followed by transposing convolutions to the desired output size in the generator. And effectively the reverse for the discriminator. All using (primarily) LeakyReLU activation and Batch Normalization (With exception of tanh for the output of the generator) This is tried and tested, however I will experiment with variations as I go along.\n",
        "\n",
        "Additionally, I will be making use of tensorboard to view losses and other network metrics. This will aid my design decisions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5GmK-bM0Pqct"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ulb56okBl9y0",
        "outputId": "5d5eeaca-b443-4dc5-b619-adef24beed54"
      },
      "source": [
        "!pip install tensorflow_io"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow_io in /usr/local/lib/python3.7/dist-packages (0.19.0)\n",
            "Requirement already satisfied: tensorflow<2.6.0,>=2.5.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow_io) (2.5.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem==0.19.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow_io) (0.19.0)\n",
            "Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6.0,>=2.5.0->tensorflow_io) (3.7.4.3)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6.0,>=2.5.0->tensorflow_io) (1.1.2)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6.0,>=2.5.0->tensorflow_io) (1.1.0)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6.0,>=2.5.0->tensorflow_io) (0.36.2)\n",
            "Requirement already satisfied: tensorboard~=2.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6.0,>=2.5.0->tensorflow_io) (2.5.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6.0,>=2.5.0->tensorflow_io) (3.17.3)\n",
            "Requirement already satisfied: grpcio~=1.34.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6.0,>=2.5.0->tensorflow_io) (1.34.1)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6.0,>=2.5.0->tensorflow_io) (1.6.3)\n",
            "Requirement already satisfied: h5py~=3.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6.0,>=2.5.0->tensorflow_io) (3.1.0)\n",
            "Requirement already satisfied: numpy~=1.19.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6.0,>=2.5.0->tensorflow_io) (1.19.5)\n",
            "Requirement already satisfied: gast==0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6.0,>=2.5.0->tensorflow_io) (0.4.0)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6.0,>=2.5.0->tensorflow_io) (3.3.0)\n",
            "Requirement already satisfied: keras-nightly~=2.5.0.dev in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6.0,>=2.5.0->tensorflow_io) (2.5.0.dev2021032900)\n",
            "Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6.0,>=2.5.0->tensorflow_io) (0.12.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.6.0,>=2.5.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6.0,>=2.5.0->tensorflow_io) (2.5.0)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6.0,>=2.5.0->tensorflow_io) (1.12.1)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6.0,>=2.5.0->tensorflow_io) (1.12)\n",
            "Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6.0,>=2.5.0->tensorflow_io) (1.15.0)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6.0,>=2.5.0->tensorflow_io) (0.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow<2.6.0,>=2.5.0->tensorflow_io) (3.3.4)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow<2.6.0,>=2.5.0->tensorflow_io) (1.8.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow<2.6.0,>=2.5.0->tensorflow_io) (57.0.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow<2.6.0,>=2.5.0->tensorflow_io) (2.23.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow<2.6.0,>=2.5.0->tensorflow_io) (1.0.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow<2.6.0,>=2.5.0->tensorflow_io) (0.6.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow<2.6.0,>=2.5.0->tensorflow_io) (0.4.4)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow<2.6.0,>=2.5.0->tensorflow_io) (1.32.1)\n",
            "Requirement already satisfied: cached-property; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from h5py~=3.1.0->tensorflow<2.6.0,>=2.5.0->tensorflow_io) (1.5.2)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.5->tensorflow<2.6.0,>=2.5.0->tensorflow_io) (4.6.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow<2.6.0,>=2.5.0->tensorflow_io) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow<2.6.0,>=2.5.0->tensorflow_io) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow<2.6.0,>=2.5.0->tensorflow_io) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow<2.6.0,>=2.5.0->tensorflow_io) (1.24.3)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow<2.6.0,>=2.5.0->tensorflow_io) (1.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow<2.6.0,>=2.5.0->tensorflow_io) (4.7.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow<2.6.0,>=2.5.0->tensorflow_io) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow<2.6.0,>=2.5.0->tensorflow_io) (4.2.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard~=2.5->tensorflow<2.6.0,>=2.5.0->tensorflow_io) (3.4.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow<2.6.0,>=2.5.0->tensorflow_io) (3.1.1)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.7/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3.6\"->google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow<2.6.0,>=2.5.0->tensorflow_io) (0.4.8)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KmGm78MDM5cQ"
      },
      "source": [
        "#Misc\n",
        "from datetime import datetime\n",
        "from IPython import display\n",
        "import os\n",
        "import time\n",
        "from scipy.io.wavfile import write\n",
        "\n",
        "\n",
        "#ML\n",
        "import tensorflow as tf\n",
        "import tensorflow_io as tfio\n",
        "import numpy as np\n",
        "import librosa \n",
        "\n",
        "#Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.utils import plot_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z6RnDF97JCwh"
      },
      "source": [
        "%load_ext tensorboard\n",
        "!rm -rf ./logs/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TUvZVff8JUbS"
      },
      "source": [
        "log_dir = \"logs/log\"\n",
        "writer = tf.summary.create_file_writer(log_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zPwJgIX9P11c"
      },
      "source": [
        "\n",
        "# Loading the datasets\n",
        "This system uses the *Drum Sound Effects dataset* from ***Donahue, C., J. McAuley, and M. Puckette. 2019. \"Adversarial Audio Synthesis.\"***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JpOx7RuVykgK"
      },
      "source": [
        "BUFFER_SIZE = 60000\n",
        "BATCH_SIZE = 32"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Njx-LnEGOPBI"
      },
      "source": [
        "drum_sounds_file = tf.keras.utils.get_file(\n",
        "    \"drum_sounds\",\n",
        "    \"http://deepyeti.ucsd.edu/cdonahue/wavegan/data/drums.tar.gz\",\n",
        "    cache_dir='./',\n",
        "    extract=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CmAmgDmsPb9j"
      },
      "source": [
        "#(Taken from https://www.tensorflow.org/tutorials/audio/transfer_learning_audio)\n",
        "\n",
        "@tf.function\n",
        "def load_wav_16k_mono(filename):\n",
        "    \"\"\" Load a WAV file, convert it to a float tensor, resample to 16 kHz single-channel audio. \"\"\"\n",
        "    file_contents = tf.io.read_file(filename)\n",
        "    wav, sample_rate = tf.audio.decode_wav(\n",
        "          file_contents,\n",
        "          desired_channels=1)\n",
        "    wav = tf.squeeze(wav, axis=-1)\n",
        "    sample_rate = tf.cast(sample_rate, dtype=tf.int64)\n",
        "    wav = tfio.audio.resample(wav, rate_in=sample_rate, rate_out=16000)\n",
        "    return wav"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SICey3_d18Dp"
      },
      "source": [
        "def load_dataset():\n",
        "  i = 0\n",
        "  filenames = []\n",
        "\n",
        "  for subdir, dirs, files in os.walk(\"datasets/drums/train\"): \n",
        "      for file in files:\n",
        "          if \"Snare\" in file:\n",
        "            filenames.append(\"datasets/drums/train/\" + file)\n",
        "          \n",
        "  print(\"Loaded \" + str(len(filenames)) + \" samples\")\n",
        "\n",
        "  dataset = tf.data.Dataset.from_tensor_slices(filenames)\n",
        "  dataset = dataset.map(load_wav_16k_mono)\n",
        "\n",
        "  return dataset\n",
        "  \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aWo-ejd_1wnb"
      },
      "source": [
        "## Processing the dataset\n",
        "We need to move our wav files into the frequency domain using STFTs\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "To do this, we create a function which converts a signal to it's composing STFT slices, as shown below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W3mPmv6kFEXF"
      },
      "source": [
        "length = 16384\n",
        "\n",
        "def waveform_to_stft(waveform):\n",
        "  #Pad with zeroes for short waveforms \n",
        "  #zero_padding = tf.zeros([length] - tf.shape(waveform), dtype=tf.float32)\n",
        "  slices = tf.signal.frame(waveform, frame_length = length, frame_step=length, pad_end=True)\n",
        "  slices = tf.reshape(slices, shape=[length,])\n",
        "\n",
        "  #Truncate longer waveforms\n",
        "  #waveform = tf.cast(waveform, tf.float32)\n",
        "  #equal_length = tf.concat([waveform, zero_padding], 0)\n",
        "\n",
        "  slices = tf.signal.stft(slices, 128, 128)\n",
        "\n",
        "  slices = slices[:128,:64]\n",
        "  slices = tf.reshape(slices, shape=[128,64])\n",
        "  slices = abs(slices)\n",
        "  return slices / tf.norm(slices)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VDjwIGD2-Qea"
      },
      "source": [
        "dataset = load_dataset()\n",
        "\n",
        "#Convert waveforms to Frequency domain\n",
        "dataset = dataset.map(waveform_to_stft)\n",
        "\n",
        "dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE).prefetch(tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "\n",
        "\n",
        "print(len(dataset))\n",
        "print(dataset.element_spec)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dGwiOresUcgc"
      },
      "source": [
        "# Creating the models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ui2yID8XZKrU"
      },
      "source": [
        "## The Generator\n",
        "The generator takes a tensor of normally distributed noise and outputs a 128x128x1 tensor representing 128 fourier series' each with 128 terms. It does this using 6 convolution layers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2YLz0mcqUi3z"
      },
      "source": [
        "input_shape = [100,]\n",
        "\n",
        "def create_generator():\n",
        "  model = tf.keras.Sequential()\n",
        "\n",
        "  #Input layer\n",
        "  model.add(tf.keras.layers.InputLayer(input_shape))\n",
        "  model.add(tf.keras.layers.Reshape([10,10,1]))\n",
        "\n",
        "  model.add(tf.keras.layers.Conv2DTranspose(32, 3, 2, padding='same'))\n",
        "  model.add(tf.keras.layers.BatchNormalization())\n",
        "  model.add(tf.keras.layers.Reshape([16,16,50]))\n",
        "\n",
        "  model.add(tf.keras.layers.Conv2DTranspose(30, 3, 2, padding='same'))\n",
        "  model.add(tf.keras.layers.BatchNormalization())\n",
        "  model.add(tf.keras.layers.Reshape([32,32,30]))\n",
        "\n",
        "  model.add(tf.keras.layers.Conv2DTranspose(30, 3, 2, padding='same'))\n",
        "  model.add(tf.keras.layers.BatchNormalization())\n",
        "  model.add(tf.keras.layers.Reshape([64,64,30]))\n",
        "\n",
        "  model.add(tf.keras.layers.Conv2DTranspose(30, 3, (2,1), padding='same'))\n",
        "  model.add(tf.keras.layers.BatchNormalization())\n",
        "  model.add(tf.keras.layers.Reshape([128,64,30])) \n",
        "\n",
        "  model.add(tf.keras.layers.Dense(16))\n",
        "  model.add(tf.keras.layers.Dropout(0.5))\n",
        "  model.add(tf.keras.layers.LeakyReLU())\n",
        "\n",
        "  model.add(tf.keras.layers.Dense(8))\n",
        "  model.add(tf.keras.layers.Dropout(0.3))\n",
        "  model.add(tf.keras.layers.LeakyReLU())\n",
        "\n",
        "  model.add(tf.keras.layers.Conv2DTranspose(1, 3, 1, padding='same'))\n",
        "\n",
        "  model.add(tf.keras.layers.Activation('sigmoid'))\n",
        "\n",
        "  model.add(tf.keras.layers.Reshape(target_shape=[128,64,1]))\n",
        "  assert model.output_shape == (None, 128, 64, 1)\n",
        "\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v-s30iStZfsq"
      },
      "source": [
        "### Testing the generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CGhagD-dZF3z"
      },
      "source": [
        "#Create the generator\n",
        "generator = create_generator()\n",
        "plot_model(generator, show_shapes=True)\n",
        "\n",
        "#Create noise\n",
        "noise = tf.random.normal([1, 100])\n",
        "generated_sound = generator(noise, training=False)\n",
        "\n",
        "#Outputs as expected (Noise for now as network is untrained)\n",
        "plt.plot(generated_sound[0,:,:,0])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xoa3EnLNhHRN"
      },
      "source": [
        "### Issues with STFT Representation\n",
        "Currently the network outputs a STFT Tensor. This is where small slices of the sound are taken and transformed to frequency domain, as opposed to the signal as a whole. This offers a representation of how the sound changes with time and allows a smaller, lower-quality representation. It is what is usually used to create a spectrogram, which is what you typically associate with music in the frequency domain and allows more useful insight into music than a fourier transform applied to the entire waveform.\n",
        "\n",
        "The problem with STFT is that it is not possible to directly convert a magnitude only spectrogram back to it's original signal without use of an algorithm. As such, I will make use of the *Griffin-Lim Algorithm*. My system will use *Librosa*'s implementation "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r9iE0z83j-Wp"
      },
      "source": [
        "#Griffin-Lim reconstruction (https://librosa.org/doc/latest/generated/librosa.griffinlim.html)\n",
        "\n",
        "stft = next(iter(dataset))\n",
        "\n",
        "\n",
        "reconstructed_sound = to_complex_signal(stft)\n",
        "plt.subplot(1,3,1)\n",
        "plt.plot(reconstructed_sound)\n",
        "plt.subplot(1,3,2)\n",
        "plt.plot(stft[:,:])\n",
        "plt.subplot(1,3,3)\n",
        "plt.plot(load_wav_16k_mono(\"datasets/drums/train/Snare_00623.wav\"))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-LXqObokrtG"
      },
      "source": [
        "reconstructed_sound = to_complex_signal(generated_sound[0,:,:,0])\n",
        "\n",
        "plt.subplot(1,2,1)\n",
        "plt.plot(reconstructed_sound)\n",
        "plt.subplot(1,2,2)\n",
        "plt.plot(generated_sound[0,:,:,0])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1pyYlAaItvUf"
      },
      "source": [
        "def to_complex_signal(input_sound):\n",
        "    out = librosa.griffinlim(np.array(input_sound), n_iter=5, win_length=32, hop_length=128)\n",
        "    out = tf.reshape(out, [len(out),1])\n",
        "    out = out.numpy()\n",
        "    return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oTH2ruIjuQQn"
      },
      "source": [
        "## The discriminator\n",
        "The discriminator takes a 128x128x1 Tensor and outputs a Tensor which encodes how confident the discriminator is that the input belongs to the dataset. This is our adversary. The discriminator and generator will 'compete', the generator trying to fool the discriminator and the discriminator trying to catch out the generator. As such, over time, both should get very good at their jobs.\n",
        "\n",
        "We will track their losses using TensorBoard and should see that they diverge, hopefully in the generator's favour"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XtgpufK2u-QE"
      },
      "source": [
        "def create_discriminator():\n",
        "  model = tf.keras.Sequential();\n",
        "\n",
        "  #Input layer\n",
        "  model.add(tf.keras.layers.InputLayer([128, 64, 1]))\n",
        "  assert model.output_shape == (None, 128, 64, 1)\n",
        "\n",
        "  #Convolution\n",
        "  model.add(tf.keras.layers.Conv2D(30, 3, 1, padding='same'))\n",
        "  model.add(tf.keras.layers.MaxPooling2D())\n",
        "  model.add(tf.keras.layers.LeakyReLU())\n",
        "\n",
        "  #Convolution\n",
        "  model.add(tf.keras.layers.Conv2D(30, 3, 1, padding='same'))\n",
        "  model.add(tf.keras.layers.MaxPooling2D())\n",
        "  model.add(tf.keras.layers.LeakyReLU())\n",
        "\n",
        "  model.add(tf.keras.layers.Dense(64))\n",
        "  model.add(tf.keras.layers.Dropout(0.5))\n",
        "  model.add(tf.keras.layers.LeakyReLU())\n",
        "\n",
        "  model.add(tf.keras.layers.Dense(4))\n",
        "  model.add(tf.keras.layers.Dropout(0.3))\n",
        "  model.add(tf.keras.layers.LeakyReLU())\n",
        "\n",
        "  model.add(tf.keras.layers.Flatten())\n",
        "  model.add(tf.keras.layers.Dense(1024))\n",
        "  model.add(tf.keras.layers.Dropout(0.3))\n",
        "  model.add(tf.keras.layers.LeakyReLU())\n",
        "\n",
        "  #Final dense layer to determine output\n",
        "  model.add(tf.keras.layers.Flatten())\n",
        "  model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47dL1DkCwPgq"
      },
      "source": [
        "### Testing the discriminator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xJZo77PCwR8-"
      },
      "source": [
        "#Create the discriminator\n",
        "discriminator = create_discriminator()\n",
        "\n",
        "decision = discriminator(generated_sound)\n",
        "print(decision)\n",
        "plot_model(discriminator, show_shapes=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VitqUQBFwvsa"
      },
      "source": [
        "# Defining loss functions\n",
        "(https://www.tensorflow.org/tutorials/generative/dcgan#define_the_loss_and_optimizers)\n",
        "\n",
        "\n",
        "A GAN is 'adversarial' by nature, in that the loss of the generator will be higher when the loss of the discriminator is lower. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "beIGPzv9wz2S"
      },
      "source": [
        "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "\n",
        "generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
        "discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NOveA7ygw2Dv"
      },
      "source": [
        "## Discriminator loss\n",
        "The discriminator loss function will compare the discriminator's predictions for real and fake images against the true value. The discriminator does well if it accuratly predicts which images are 'real' (from the dataset) and which are 'fake' (from the generator)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wUBIZPihw4UM"
      },
      "source": [
        "def discriminator_loss(real_output, fake_output):\n",
        "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
        "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
        "    total_loss = real_loss + fake_loss\n",
        "    return total_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0wOR48K1w5Ks"
      },
      "source": [
        "## Generator loss\n",
        "The generator loss function compares the discriminator's predictions for fake images against the actual fake images. If the discriminator fails to categorize fake images as fake, the generator's loss function will be lower (The generator did a good job)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yjWBlOZdw6b_"
      },
      "source": [
        "def generator_loss(fake_output):\n",
        "    return cross_entropy(tf.ones_like(fake_output), fake_output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Muhd7ruRPRj"
      },
      "source": [
        "# One last test...\n",
        "\n",
        "Now let's test the system as a whole, obviously it won't be very good as we haven't trained it yet but let's make sure our networks are designed correctly and can process the data properly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l8IBPWrkRbfN"
      },
      "source": [
        "noise = tf.random.normal([1, 100])\n",
        "\n",
        "generated_sample = generator(noise, training=False)\n",
        "real_output = discriminator(generated_sample, training=False)\n",
        "\n",
        "print(real_output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aMjYRd_DySfb"
      },
      "source": [
        "# Creating the training loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MlzHrl82yTts"
      },
      "source": [
        "EPOCHS = 30"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HpvIHwJw1ThR"
      },
      "source": [
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
        "                                 discriminator_optimizer=discriminator_optimizer,\n",
        "                                 generator=generator,\n",
        "                                 discriminator=discriminator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mLtvou12qsF-"
      },
      "source": [
        "def write_sound(signal, filename):\n",
        "  write(filename, 16000, signal)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "28osYFyP0Z5r"
      },
      "source": [
        "def draw_sound(model, epoch, test_input):\n",
        "  # Inference Mode (training=False)\n",
        "  prediction = model(test_input, training=False)\n",
        "\n",
        "  #Convert to time domain\n",
        "  reconstructed_sound = to_complex_signal(prediction[0,:,:,0])\n",
        "\n",
        "  plt.plot(reconstructed_sound)\n",
        "  plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ok5hOoulJsis"
      },
      "source": [
        "generator_loss_metric = tf.keras.metrics.Mean('generator_loss', dtype=tf.float32)\n",
        "discriminator_loss_metric = tf.keras.metrics.Mean('discriminator_loss', dtype=tf.float32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cVipQPpLye96"
      },
      "source": [
        "#(Taken from https://www.tensorflow.org/tutorials/generative/dcgan#define_the_training_loop)\n",
        "\n",
        "@tf.function\n",
        "def train_step(images):\n",
        "    noise = tf.random.normal([BATCH_SIZE, 100])\n",
        "\n",
        "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
        "      generated_images = generator(noise, training=True)\n",
        "\n",
        "      real_output = discriminator(images, training=True)\n",
        "      fake_output = discriminator(generated_images, training=True)\n",
        "\n",
        "      gen_loss = generator_loss(fake_output)\n",
        "      disc_loss = discriminator_loss(real_output, fake_output)\n",
        "      \n",
        "      generator_loss_metric(gen_loss)\n",
        "      discriminator_loss_metric(disc_loss)\n",
        "\n",
        "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
        "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
        "\n",
        "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
        "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fRMKHY7fzFiz"
      },
      "source": [
        "def train(dataset):\n",
        "  seed = tf.random.normal([1, 100])\n",
        "  for epoch in range(EPOCHS):\n",
        "    start = time.time()\n",
        "    for batch in dataset:\n",
        "      #print(image_batch)\n",
        "      batch = tf.reshape(batch, [1,128,64,1])\n",
        "      train_step(batch)\n",
        "\n",
        "    with writer.as_default():\n",
        "      tf.summary.scalar('generator loss', generator_loss_metric.result(), step=epoch)\n",
        "      tf.summary.scalar('discriminator loss', discriminator_loss_metric.result(), step=epoch)\n",
        "\n",
        "    display.clear_output(wait=True)\n",
        "\n",
        "    stft = generator(seed, training=False)\n",
        "\n",
        "    reconstructed_sound = (to_complex_signal(stft[0,:,:,0]) * 255).astype(np.float32)\n",
        "    plt.subplot(1,2,1)\n",
        "    plt.plot(reconstructed_sound)\n",
        "    plt.subplot(1,2,2)\n",
        "    plt.plot(stft[0,:,:,0])\n",
        "    plt.show()\n",
        "\n",
        "    write_sound(reconstructed_sound, \"epoch\" + str(epoch) + \".wav\")\n",
        "\n",
        "    checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "    generator_loss_metric.reset_states()\n",
        "    discriminator_loss_metric.reset_states()\n",
        "\n",
        "    print ('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IvMHV7oo1rJS"
      },
      "source": [
        "## Time to train!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4OjmYZF3LCpD"
      },
      "source": [
        "%tensorboard --logdir logs/log"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uYlZ07n01sNi"
      },
      "source": [
        "train(dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pJqhsoiREV3H"
      },
      "source": [
        "noise = tf.random.normal([1, 100])\n",
        "stft = generator(noise, training=False)\n",
        "\n",
        "reconstructed_sound = (to_complex_signal(stft[0,:,:,0]) * 255).astype(np.float32)\n",
        "plt.subplot(1,2,1)\n",
        "plt.plot(reconstructed_sound)\n",
        "plt.subplot(1,2,2)\n",
        "plt.plot(stft[0,:,:,0])\n",
        "plt.show()\n",
        "\n",
        "write_sound(reconstructed_sound, \"finalOutput.wav\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}